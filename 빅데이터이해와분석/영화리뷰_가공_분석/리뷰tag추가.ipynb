{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e6b9ba8",
   "metadata": {},
   "source": [
    "### ì˜í™” ë¦¬ë·°ì— ë”°ë¼ tagë¥¼ 10ê°œ ì •ë„ ì¶”ì¶œí•˜ëŠ” ëª¨ë¸ ì œì‘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62734c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ê°€ì°½ë ¥ ì—°ì£¼ìì˜', 'ì—°ì¶œ ì§„ì§€í•´ìš”', 'ì—°ì¶œê³¼ ê°€ì°½ë ¥', 'ì—°ì£¼ì ë…íŠ¹í•˜êµ¬ìš”', 'ê¸°ë°˜ìœ¼ë¡œ ì—°ì¶œ', 'ì—¬ì ì—°ì¶œê³¼', 'í”¼ê°€ë¡œì—­ ì—°ì£¼ì', 'ì—°ì¶œ ë³¼ë§Œí•©ë‹ˆë‹¤', 'ë…íŠ¹í•˜êµ¬ìš” íƒ„íƒ„í•œ', 'ë³¼ë§Œí•©ë‹ˆë‹¤ í”¼ê°€ë¡œì—­', 'ì—°ì£¼ìì˜ ì†Œë¦¬', 'ì§„ì§€í•´ìš” ì—¬ì', 'ë¬´ëŒ€ ì—°ì¶œ', 'ì—°ì¶œê³¼', 'ì—°ì£¼ìì˜', 'ì§„ì§€í•´ìš”', 'ì—°ì¶œ', 'ë…íŠ¹í•˜êµ¬ìš”', 'ì—°ì£¼ì', 'ê°€ì°½ë ¥', 'ë„ˆë¬´ ì§„ì§€', 'ë³¼ë§Œí•©ë‹ˆë‹¤', 'ì§„ì§€ ë¬´ëŒ€', 'í”¼ê°€ë¡œì—­', 'ì†Œë¦¬ ê¸°ë°˜ìœ¼ë¡œ', 'ì—¬ì', 'ê¸°ë°˜ìœ¼ë¡œ', 'íƒ„íƒ„í•œ', 'ì§„ì§€', 'ì†Œë¦¬']\n"
     ]
    }
   ],
   "source": [
    "#keybert í…ŒìŠ¤íŠ¸\n",
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "reviews = [\n",
    "    'ë„ˆë¬´ ì§„ì§€', \n",
    "    'ë¬´ëŒ€ ì—°ì¶œ', \n",
    "    'ì§„ì§€í•´ìš” ì—¬ì',\n",
    "    'ì—°ì¶œê³¼ ê°€ì°½ë ¥', \n",
    "    'ì—°ì£¼ìì˜ ì†Œë¦¬', \n",
    "    'ê¸°ë°˜ìœ¼ë¡œ ì—°ì¶œ', \n",
    "    'ë³¼ë§Œí•©ë‹ˆë‹¤', \n",
    "    'í”¼ê°€ë¡œì—­ ì—°ì£¼ì', \n",
    "    'ë…íŠ¹í•˜êµ¬ìš” íƒ„íƒ„í•œ'\n",
    "    ]\n",
    "\n",
    "\n",
    "full_text = \" \".join(reviews)\n",
    "keywords = kw_model.extract_keywords(full_text, top_n=30, keyphrase_ngram_range=(1, 2))\n",
    "\n",
    "print([kw[0] for kw in keywords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98fe4747",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = kw_model.extract_keywords(full_text, top_n=7, keyphrase_ngram_range=(1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e013895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì—°ì¶œê³¼', 'ì—°ì£¼ìì˜', 'ì§„ì§€í•´ìš”', 'ì—°ì¶œ', 'ë…íŠ¹í•˜êµ¬ìš”', 'ì—°ì£¼ì', 'ê°€ì°½ë ¥']\n"
     ]
    }
   ],
   "source": [
    "print([kw[0] for kw in keywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b24b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\term_pro\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\term_pro\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "ğŸ‰ ìµœì¢… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ â†’ megabox_final_keywords_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv(\"1.megabox_reviews_merged_avg.csv\")\n",
    "review_cols = [\"ë¦¬ë·°1\", \"ë¦¬ë·°2\", \"ë¦¬ë·°3\", \"ë¦¬ë·°4\", \"ë¦¬ë·°5\"]\n",
    "df[\"ì „ì²´ë¦¬ë·°\"] = df[review_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "# ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "kw_model = KeyBERT()\n",
    "okt = Okt()\n",
    "\n",
    "# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def extract_clean_keywords(text):\n",
    "    try:\n",
    "        keywords = kw_model.extract_keywords(text, top_n=10, keyphrase_ngram_range=(1, 2))\n",
    "        phrases = [kw[0] for kw in keywords]\n",
    "\n",
    "        refined = []\n",
    "        for phrase in phrases:\n",
    "            tokens = okt.pos(phrase, stem=True)\n",
    "            for word, tag in tokens:\n",
    "                if tag in [\"Noun\", \"Adjective\", \"Verb\"] and len(word) > 1:\n",
    "                    refined.append(word)\n",
    "        return \" \".join(refined)\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "# ì •ì œ í‚¤ì›Œë“œ ë¬¸ì¥ ìƒì„±\n",
    "df[\"ì •ì œë‹¨ì–´ë¬¸ì¥\"] = df[\"ì „ì²´ë¦¬ë·°\"].apply(extract_clean_keywords)\n",
    "\n",
    "# TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"ì •ì œë‹¨ì–´ë¬¸ì¥\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def get_top_tfidf_keywords(row_idx, top_n=5):\n",
    "    row = tfidf_matrix[row_idx]\n",
    "    scores = row.toarray().flatten()\n",
    "    top_indices = scores.argsort()[-top_n:][::-1]\n",
    "    return \", \".join([feature_names[i] for i in top_indices])\n",
    "\n",
    "df[\"ìµœì¢…í‚¤ì›Œë“œ\"] = [get_top_tfidf_keywords(i) for i in range(len(df))]\n",
    "\n",
    "# ì €ì¥\n",
    "df.to_csv(\"megabox_final_keywords_tfidf.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"ğŸ‰ ìµœì¢… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ â†’ megabox_final_keywords_tfidf.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
